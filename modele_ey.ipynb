{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b73dc2-18ec-4481-8a05-c8b9b93ebde9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Phase 1: Tropical Cyclone Damage Assesment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd4f124-29c9-4ced-bed8-4dbabc5443c6",
   "metadata": {},
   "source": [
    "## Challenge Phase 1 Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a35c1c7c-30d1-4672-ac4b-ac7dfc5ce463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GeoTiff Images\n",
    "#from osgeo import gdal\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from matplotlib.pyplot import figure\n",
    "from PIL import Image\n",
    "\n",
    "# Model Building\n",
    "from ultralytics import YOLO\n",
    "#import yolov9\n",
    "import labelme2yolo\n",
    "\n",
    "# Others\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616c86b2-8039-405a-bec7-f0adafb5fbc9",
   "metadata": {},
   "source": [
    "## Data Pre Processing "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdad7d25",
   "metadata": {},
   "source": [
    "### Tiles generation\n",
    "\n",
    "This function generate tiles from an image. It can be used on only a part of the image, defined by start_x_prop, start_y_prop, end_x_prop and end_y_prop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe5be0d8-3f39-48ea-858b-1bee355e2a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tiles(input_file, output_dir, grid_x, grid_y, start_x_prop, start_y_prop, end_x_prop, end_y_prop, id):\n",
    "    ds = gdal.Open(input_file)\n",
    "\n",
    "\n",
    "    # Get image size and number of bands\n",
    "    width = ds.RasterXSize\n",
    "    height = ds.RasterYSize\n",
    "    num_bands = ds.RasterCount\n",
    "\n",
    "    start_x = int(start_x_prop*width)\n",
    "    end_x = int(end_x_prop*width)\n",
    "    start_y = int(start_y_prop*height)\n",
    "    end_y = int(end_y_prop*height)\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate number of tiles in each dimension within the specified area\n",
    "    num_tiles_x = ((end_x - start_x) // grid_x) + 1\n",
    "    num_tiles_y = ((end_y - start_y) // grid_y) + 1\n",
    "\n",
    "    print(f\"Total number of tiles: {num_tiles_x * num_tiles_y}\")\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate over each tile and save as a separate TIFF image\n",
    "    for i in range(num_tiles_x):\n",
    "        for j in range(num_tiles_y):\n",
    "            x_offset = start_x + (i * grid_x)\n",
    "            y_offset = start_y + (j * grid_y)\n",
    "\n",
    "            # Ensure tile doesn't exceed image bounds\n",
    "            if x_offset >= end_x or y_offset >= end_y:\n",
    "                continue\n",
    "\n",
    "            tile_width = min(grid_x, end_x - x_offset)\n",
    "            tile_height = min(grid_y, end_y - y_offset)\n",
    "\n",
    "            tile = []\n",
    "            for band in range(1, num_bands + 1):\n",
    "                tile_data = ds.GetRasterBand(band).ReadAsArray(x_offset, y_offset, tile_width, tile_height)\n",
    "                tile.append(tile_data)\n",
    "\n",
    "            # Create output filename\n",
    "            output_file = os.path.join(output_dir, f\"tile_{id}_{i}_{j}.tif\")\n",
    "\n",
    "            # Create an output TIFF file with same CRS and band values range\n",
    "            driver = gdal.GetDriverByName(\"GTiff\")\n",
    "            options = ['COMPRESS=DEFLATE', 'PREDICTOR=2', 'TILED=YES']\n",
    "            out_ds = driver.Create(output_file, tile_width, tile_height, num_bands, \n",
    "                                    ds.GetRasterBand(1).DataType, options=options)\n",
    "\n",
    "            # Set the geotransform\n",
    "            geotransform = list(ds.GetGeoTransform())\n",
    "            geotransform[0] = geotransform[0] + x_offset * geotransform[1]\n",
    "            geotransform[3] = geotransform[3] + y_offset * geotransform[5]\n",
    "            out_ds.SetGeoTransform(tuple(geotransform))\n",
    "\n",
    "            # Set the projection\n",
    "            out_ds.SetProjection(ds.GetProjection())\n",
    "\n",
    "            # Write each band to the output file\n",
    "            for band in range(1, num_bands + 1):\n",
    "                out_band = out_ds.GetRasterBand(band)\n",
    "                out_band.WriteArray(tile[band - 1])\n",
    "\n",
    "            # Close the output file\n",
    "            out_ds = None\n",
    "\n",
    "    print(\"Tiles generation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fff7546-6c3a-481c-817d-551f73ce2570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tiles: 2775\n",
      "Tiles generation completed.\n"
     ]
    }
   ],
   "source": [
    "input_file = \"/Users/homefolder/Desktop/Challenge_EY/Post_Event_San_Juan.tif\"\n",
    "output_dir_1 = \"/Users/homefolder/Desktop/Challenge_EY/Post_Event_Grids_In_TIFF_1\"\n",
    "output_dir_2 = \"/Users/homefolder/Desktop/Challenge_EY/Post_Event_Grids_In_TIFF_2\"\n",
    "output_dir_3 = \"/Users/homefolder/Desktop/Challenge_EY/Post_Event_Grids_In_TIFF_3\"\n",
    "\n",
    "\n",
    "grid_x = 512\n",
    "grid_y = 512\n",
    "generate_tiles(input_file, output_dir_1,grid_x,grid_y, 0.333, 0, 1, 0.333, 1)\n",
    "generate_tiles(input_file, output_dir_2,grid_x,grid_y, 0, 0.8, 1, 1, 2)\n",
    "generate_tiles(input_file, output_dir_3,grid_x,grid_y, 0, 0.25, 1, 0.5, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eea27ff5-d4ea-4027-aafc-f85e9fe57f61",
   "metadata": {},
   "source": [
    "## Labelling\n",
    "\n",
    "This function is used to convert TIFF images to JPEG images to label them with LabelMe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "592c7fa0-91fa-4982-9a4f-d145a56df2c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_tiff_to_jpeg(input_dir,output_dir):\n",
    "    # check if output_dir exists, if not create it\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        # check if file is an image (ends with .tif)\n",
    "        if filename.endswith('.tif'):\n",
    "            img = Image.open(os.path.join(input_dir, filename))\n",
    "        \n",
    "            # check if image is RGB mode, if not convert it\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "        \n",
    "            # create new filename, replace .tif with .jpg\n",
    "            output_filename = os.path.splitext(filename)[0] + '.jpg'\n",
    "        \n",
    "            # save the image in JPEG format\n",
    "            img.save(os.path.join(output_dir, output_filename), 'JPEG')\n",
    "    print(\"Conversion from TIFF to JPEG completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40cc908c-5ac1-4544-bc63-9dfbdc7e6da1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion from TIFF to JPEG completed.\n"
     ]
    }
   ],
   "source": [
    "# specify directory\n",
    "input_dir_1 = \"/Users/homefolder/Desktop/Challenge_EY/Post_Event_Grids_In_TIFF_1\"\n",
    "output_dir_1 = \"/Users/homefolder/Desktop/Challenge_EY/Post_Event_Grids_In_JPEG_1\"\n",
    "input_dir_2 = \"/Users/homefolder/Desktop/Challenge_EY/Post_Event_Grids_In_TIFF_2\"\n",
    "output_dir_2 = \"/Users/homefolder/Desktop/Challenge_EY/Post_Event_Grids_In_JPEG_2\"\n",
    "input_dir_3 = './Post_Event_Grids_In_TIFF_3'\n",
    "output_dir_3 = './Post_Event_Grids_In_JPEG_3'\n",
    "\n",
    "convert_tiff_to_jpeg(input_dir_1,output_dir_1)\n",
    "convert_tiff_to_jpeg(input_dir_2,output_dir_2)\n",
    "convert_tiff_to_jpeg(input_dir_3,output_dir_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d3e64a-3fcd-4f4c-8ee4-7759f2756e22",
   "metadata": {},
   "source": [
    "### Renaming the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75463ed9-0800-42c9-af4a-3e2babd916b6",
   "metadata": {},
   "source": [
    "<div align = \"justify\">For easier and more efficient data accessibility, it's necessary to rename the files in the directory. We'll use the function <b><i>rename_files</b></i> to accomplish this task of altering the file names in the given path.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75595d54-1bba-4e8f-b8b3-3d7eec05b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files(directory_path, id):\n",
    "# Define the directory path where your files are located\n",
    "    directory_path = directory_path\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory_path)\n",
    "    \n",
    "    # Define a prefix for the new file names \n",
    "    prefix = f\"Post_Event_{id}_\"\n",
    "    \n",
    "    # Start the numbering from 1\n",
    "    number = 0\n",
    "    \n",
    "    # Loop through each file in the directory\n",
    "    for filename in files:\n",
    "        # Check if the item is a file (not a directory)\n",
    "        if os.path.isfile(os.path.join(directory_path, filename)):\n",
    "            # Get the file extension\n",
    "            file_extension = os.path.splitext(filename)[1]\n",
    "    \n",
    "            # Create the new file name with leading zeros\n",
    "            new_filename = f\"{prefix}{number:03}{file_extension}\"\n",
    "    \n",
    "            # Construct the full path to the original and new files\n",
    "            old_filepath = os.path.join(directory_path, filename)\n",
    "            new_filepath = os.path.join(directory_path, new_filename)\n",
    "    \n",
    "            # Rename the file\n",
    "            os.rename(old_filepath, new_filepath)\n",
    "    \n",
    "            # Increment the number for the next file\n",
    "            number += 1\n",
    "    \n",
    "    print(\"Files renamed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b23ce461-6a16-4585-bed2-6733ec3aebd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files renamed successfully.\n",
      "Files renamed successfully.\n",
      "Files renamed successfully.\n"
     ]
    }
   ],
   "source": [
    "rename_files(output_dir_1, 1)\n",
    "rename_files(output_dir_2, 2)\n",
    "rename_files(output_dir_3, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e123682e-8ef8-45da-8130-9450b6601efe",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "<div align=\"justify\">\n",
    "\n",
    "My model is based on the following idea : divide to conquer. Therefore, it first tries to estimate if the image is from the countryside, the suburbs, or the city center. I believe those three categories are the most prominent ones in the dataset.\n",
    "The classifition is made not by using a CNN model but simply by finding the proportion of green and brown pixels. It is far from being perfect, but I belive it will still make it easier for the model to learn."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "846bde60-6841-4742-b94a-834e3bfc800e",
   "metadata": {},
   "source": [
    "### Image Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "612bf80a",
   "metadata": {},
   "source": [
    "This function is used to determine if the given image is from the countryside, the suburbs, or the city center. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87cfb049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_area_type(input_folder, filename):\n",
    "\n",
    "    basename, extension = os.path.splitext(filename)\n",
    "\n",
    "    # Lire l'image\n",
    "    img_path = os.path.join(input_folder, filename)\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    # Convertir l'image en espace de couleur HSV\n",
    "    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Définir les plages de couleur verte et marron\n",
    "    lower_green = np.array([40, 20, 10])\n",
    "    upper_green = np.array([165, 255, 254])\n",
    "    lower_brown = np.array([1, 30, 10])\n",
    "    upper_brown = np.array([40, 255, 254])\n",
    "\n",
    "    # Créer un masque pour les pixels verts et bruns\n",
    "    mask_green = cv2.inRange(img_hsv, lower_green, upper_green)\n",
    "    mask_brown = cv2.inRange(img_hsv, lower_brown, upper_brown)\n",
    "\n",
    "    total_pixels = img.shape[0] * img.shape[1]\n",
    "\n",
    "    # Calculer le pourcentage de pixels verts et bruns\n",
    "    green_percentage = cv2.countNonZero(mask_green) / total_pixels\n",
    "    brown_percentage = cv2.countNonZero(mask_brown) / total_pixels\n",
    "\n",
    "    return green_percentage + brown_percentage\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4abd7f93",
   "metadata": {},
   "source": [
    "This function sorts the images in the given path into folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b1d6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from shutil import copy\n",
    "\n",
    "def classify_images(input_folder):\n",
    "    # Créer les dossiers de sortie s'ils n'existent pas\n",
    "    \n",
    "    output_folder_countryside = \"./images_countryside\"\n",
    "    output_folder_city = \"./images_city\"\n",
    "    output_folder_suburbs = \"./images_suburbs\"\n",
    "    os.makedirs(output_folder_countryside, exist_ok=True)\n",
    "    os.makedirs(output_folder_city, exist_ok=True)\n",
    "    os.makedirs(output_folder_suburbs, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Liste des extensions d'images supportées\n",
    "    img_extensions = ['.jpg', '.jpeg', '.png']\n",
    "\n",
    "    # Parcourir les fichiers dans le dossier d'entrée\n",
    "    for filename in os.listdir(input_folder):\n",
    "        basename, extension = os.path.splitext(filename)\n",
    "        if extension.lower() in img_extensions:\n",
    "            \n",
    "            img_path = os.path.join(input_folder, filename)\n",
    "            percentage = find_area_type(input_folder, filename)\n",
    "            \n",
    "            # Déterminer la catégorie de l'image\n",
    "            if percentage > 0.80 :\n",
    "                output_path = os.path.join(output_folder_countryside, filename)\n",
    "            elif percentage > 0.5 :\n",
    "                output_path = os.path.join(output_folder_suburbs, filename)\n",
    "            else:\n",
    "                output_path = os.path.join(output_folder_city, filename)\n",
    "\n",
    "            # Copier l'image dans le dossier de sortie correspondant\n",
    "            copy(img_path, output_path)\n",
    "\n",
    "            # Copier également le fichier .json s'il existe\n",
    "            json_path = os.path.join(input_folder, f\"{basename}.json\")\n",
    "            if os.path.exists(json_path):\n",
    "                copy(json_path, os.path.dirname(output_path))\n",
    "    print('Done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e70d97c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "classify_images('./Post_Event_Grids_In_JPEG_1/')\n",
    "classify_images('./Post_Event_Grids_In_JPEG_2/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65bd9117",
   "metadata": {},
   "source": [
    "### Labelme to Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb2d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelme_to_yolo(source, target_dir):\n",
    "    source_images_train = os.path.join(source, \"images/train\")\n",
    "    source_images_val = os.path.join(source, \"images/val\")\n",
    "    source_labels_train = os.path.join(source, \"labels/train\")\n",
    "    source_labels_val = os.path.join(source, \"labels/val\")\n",
    "    source_yaml = os.path.join(source, \"dataset.yaml\")\n",
    "    \n",
    "\n",
    "    target_images_train = os.path.join(target_dir, \"train\")\n",
    "    target_images_val = os.path.join(target_dir, \"val\")\n",
    "\n",
    "    # Supprimer les fichiers dans le répertoire cible\n",
    "    if os.path.exists(target_dir):\n",
    "        shutil.rmtree(target_dir)\n",
    "\n",
    "    # Créer les répertoires train et val\n",
    "    os.makedirs(target_images_train)\n",
    "    os.makedirs(target_images_val)\n",
    "\n",
    "    # Déplacer les fichiers d'images\n",
    "    for src, dest in [(source_images_train, target_images_train), (source_images_val, target_images_val)]:\n",
    "        for filename in os.listdir(src):\n",
    "            shutil.move(os.path.join(src, filename), dest)\n",
    "\n",
    "    # Déplacer les fichiers de labels\n",
    "    for src, dest in [(source_labels_train, target_images_train), (source_labels_val, target_images_val)]:\n",
    "        for filename in os.listdir(src):\n",
    "            shutil.move(os.path.join(src, filename), dest)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51671d08",
   "metadata": {},
   "source": [
    "I also decided to use two models instead of one. The first one is used to identify the residential and the commercial area and the second one is used to identify the damaged building inside those areas. Since I made the choice to classify the images into three categories, there are therefore six models. It does increase the number of data that needs to be labelled by two but the labelling for the first one is actually quite easy. Note that if the test images weren't 512x512 I could've trained my first model with bigger images which would have been very beneficial since the residential and commercial areas are easier to identify on a bigger scale."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7469388c",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07184a96",
   "metadata": {},
   "source": [
    "### Suburbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e023b39-8c4c-4ec0-ae25-9fa997dffafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!labelme2yolo --json_dir ./images_suburbs\n",
    "\n",
    "# Chemins des répertoires et fichiers\n",
    "source = \"./images_suburbs_verif/YOLODataset/images/\"\n",
    "target_dir = \"./datasets/datasets_suburbs\"\n",
    "\n",
    "labelme_to_yolo(source, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "772e98e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_suburbs = YOLO('./runs/detect/train76/weights/best.pt')\n",
    "model_suburbs = YOLO('runs/detect/train18/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a3bfae",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_suburbs.train(data='dataset_suburbs.yaml', epochs=300, imgsz=512, mixup=0.7, fliplr=0.5, flipud=0.5, copy_paste=0.5, auto_augment='autoaugment',patience=0) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "323b0baf",
   "metadata": {},
   "source": [
    "### City\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cc3560",
   "metadata": {},
   "outputs": [],
   "source": [
    "!labelme2yolo --json_dir ./images_city\n",
    "\n",
    "# Chemins des répertoires et fichiers\n",
    "\n",
    "source = \"./images_city/YOLODataset/\"\n",
    "target_dir = \"./datasets/datasets_city\"\n",
    "\n",
    "labelme_to_yolo(source, target_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dab008c6-ef91-46af-89a0-9d6b7f4bda17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8n summary: 225 layers, 3157200 parameters, 0 gradients, 8.9 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(225, 3157200, 0, 8.8575488)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the model\n",
    "model_city = YOLO('./runs/detect/train40/weights/best.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a3a70-bc5e-4c83-91ee-440ffe4b90bb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_city.train(data='./dataset_city.yaml', epochs=700, imgsz=512)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8c3e3d0",
   "metadata": {},
   "source": [
    "### Countryside\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0c8ff73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:labelme2yolo:Converting train set ...\n",
      "\u001b[2K\u001b[36mConverting...\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0mm \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hINFO:labelme2yolo:Converting val set ...\n",
      "\u001b[2K\u001b[36mConverting...\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0mm \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hINFO:labelme2yolo:Converting test set ...\n",
      "\u001b[2K\u001b[36mConverting...\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!labelme2yolo --json_dir ./images_countryside\n",
    "\n",
    "# Chemins des répertoires et fichiers\n",
    "\n",
    "source = \"./images_countryside/YOLODataset/\"\n",
    "target_dir = \"./datasets/images_countryside\"\n",
    "\n",
    "labelme_to_yolo(source, target_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d430d78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary: 225 layers, 3011238 parameters, 0 gradients, 8.2 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(225, 3011238, 0, 8.1952256)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_countryside = YOLO('./runs/detect/train46/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd42c0d3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_countryside = model_countryside.train(data='./dataset_countryside.yaml', epochs=30, imgsz=512, save=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "670d0c01",
   "metadata": {},
   "source": [
    "## Model 1 to Model 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa8e3330",
   "metadata": {},
   "source": [
    "This function will be used to change the hue of the identified area (residential or commercial). Unfortunatly, changing the hue is making us lose a lot of informations. Note that using RGBA and only changing the alpha channel would allow us to not lose the inforamtions linked to chromiannce but yolo only take in RGB images. However the nulber of channels can be change in the architecture of yolo, I did not have enough time to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc7d4a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def change_hue_val(image, label, points, ext):\n",
    "    # Convertir l'image en HSV\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Définir les valeurs de teinte pour chaque label\n",
    "    #if ext=='jpg':\n",
    "    hue_values = {\"residential\": 30, \"commercial\": 150}  # Jaune: 30, Violet: 150\n",
    "    #else : hue_values = {\"0\": 30, \"1\": 150}  # Jaune: 30, Violet: 150\n",
    "    hue = hue_values[label]\n",
    "\n",
    "    # Créer un masque pour le polygone\n",
    "    mask = np.zeros_like(hsv_image[:, :, 0])\n",
    "    cv2.fillPoly(mask, [points], 255)\n",
    "\n",
    "    hsv_image[:, :, 0] = np.where(mask == 255, hue, hsv_image[:, :, 0])\n",
    "    hsv_image[:, :, 1] = np.where(mask == 255, 50, hsv_image[:, :, 1])  # Saturation\n",
    "    hsv_image[:, :, 2] = np.where(mask == 100, 50, hsv_image[:, :, 2])  # Luminosité\n",
    "\n",
    "\n",
    "    # Reconvertir l'image en RGB\n",
    "    result_image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2BGR)\n",
    "    \n",
    "    return result_image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "228f15a3",
   "metadata": {},
   "source": [
    "This function find the pixels that need to be changed and then use the change_hue function to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da9aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_and_save_images_val(image_path, annotation_file, output_dir, ext):\n",
    "    # Charger l'image\n",
    "    image = cv2.imread(image_path)\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    # Lire le fichier d'annotations\n",
    "    with open(annotation_file, 'r') as file:\n",
    "        annotations = file.readlines()\n",
    "\n",
    "    # Créer une copie de l'image originale pour conserver les modifications\n",
    "    result_image = image.copy()\n",
    "\n",
    "    # Parcourir les annotations\n",
    "    for annotation in annotations:\n",
    "        annotation = annotation.strip().split()\n",
    "        label = annotation[0]\n",
    "        # Les coordonnées sont normalisées, on les reconvertit\n",
    "        points = [0,0,0,0]\n",
    "\n",
    "        points[0] = (int(float(annotation[2])), int(float(annotation[3])))\n",
    "        points[1] = (int(float(annotation[4])), int(float(annotation[3])))\n",
    "        points[2] = (int(float(annotation[4])), int(float(annotation[5])))\n",
    "        points[3] = (int(float(annotation[2])), int(float(annotation[5])))\n",
    "        \n",
    "        # Modifier la teinte des pixels dans le polygone\n",
    "        result_image = change_hue_val(result_image, label, np.array(points), ext)\n",
    "        \n",
    "    # Enregistrer l'image avec les modifications\n",
    "    filename = os.path.basename(image_path)\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    cv2.imwrite(output_path, result_image)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6705b1e",
   "metadata": {},
   "source": [
    "Finnaly this function calls the previous function and uses it on every image in the input folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b3c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_identification_val(input_dir, output_dir, ext):\n",
    "    # Parcourir les fichiers dans le répertoire des données\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            # Récupérer le chemin de l'image correspondante\n",
    "            image_path = os.path.join(input_dir, filename[:-4] + '.'+ext)\n",
    "            annotation_file = os.path.join(input_dir, filename)\n",
    "            # Vérifier si le fichier image existe et que le fichier d'annotation est valide\n",
    "            if os.path.exists(image_path):\n",
    "                # Appliquer les modifications sur l'image et l'enregistrer dans le répertoire de sortie\n",
    "                draw_and_save_images_val(image_path, annotation_file, output_dir, ext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7e7e19c-1310-4a7b-b9b1-03b9267a89ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_txt(directory, results_directory, ext, models, decoding_of_predictions):\n",
    "    model1_countryside, model1_suburbs, model1_city = models\n",
    "    \n",
    "    if os.path.exists(results_directory) : shutil.rmtree(results_directory)\n",
    "    os.mkdir(results_directory)\n",
    "    \n",
    "    # Loop through each file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Check if the current object is a file and ends with .jpeg\n",
    "        if os.path.isfile(os.path.join(directory, filename)) and filename.lower().endswith(ext):\n",
    "            # Perform operations on the file\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            print(file_path)\n",
    "            print(\"Making a prediction on \", filename)\n",
    "\n",
    "            percentage = find_area_type(directory, filename)\n",
    "            \n",
    "            # Déterminer la catégorie de l'image\n",
    "            if percentage > 0.80 :\n",
    "                results = model1_countryside.predict(file_path, save=True, iou=0.5, save_txt=True, conf=0.4)\n",
    "            elif percentage > 0.5 :\n",
    "                results = model1_suburbs.predict(file_path, save=True, iou=0.5, save_txt=True, conf=0.4)\n",
    "            else:\n",
    "                results = model1_city.predict(file_path, save=True, iou=0.5, save_txt=True, conf=0.4)\n",
    "\n",
    "            for r in results:\n",
    "    \n",
    "                conf_list = r.boxes.conf.cpu().numpy().tolist()\n",
    "                clss_list = r.boxes.cls.cpu().numpy().tolist()\n",
    "                original_list = clss_list\n",
    "                updated_list = []\n",
    "                for element in original_list:\n",
    "                        updated_list.append(decoding_of_predictions[int(element)])\n",
    "    \n",
    "            bounding_boxes = r.boxes.xyxy.cpu().numpy()\n",
    "            confidences = conf_list\n",
    "            class_names = updated_list\n",
    "    \n",
    "            # Check if bounding boxes, confidences and class names match\n",
    "            if len(bounding_boxes) != len(confidences) or len(bounding_boxes) != len(class_names):\n",
    "                print(\"Error: Number of bounding boxes, confidences, and class names should be the same.\")\n",
    "                continue\n",
    "            \n",
    "            text_file_name = os.path.splitext(filename)[0]\n",
    "            # Creating a new .txt file for each image in the submission_directory\n",
    "            with open(os.path.join(results_directory, f\"{text_file_name}.txt\"), \"w\") as file:\n",
    "                for i in range(len(bounding_boxes)):\n",
    "                    # Get coordinates of each bounding box\n",
    "                    left, top, right, bottom = bounding_boxes[i]\n",
    "                    # Write content to file in desired format\n",
    "                    file.write(f\"{class_names[i]} {confidences[i]} {left} {top} {right} {bottom}\\n\")\n",
    "            print(\"Output files generated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93ee9e36-a121-477f-b906-b867028cc79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_images(source_dir, destination_dir, ext):\n",
    "    \n",
    "    for filename in os.listdir(source_dir):\n",
    "        source_file = os.path.join(source_dir, filename)\n",
    "        # Vérifie si le fichier est un fichier PNG\n",
    "        if os.path.isfile(source_file) and filename.lower().endswith(f'.{ext}'):\n",
    "            # Copie le fichier vers le dossier de destination\n",
    "            destination_file = os.path.join(destination_dir, filename)\n",
    "            shutil.copy(source_file, destination_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84ca1bf0",
   "metadata": {},
   "source": [
    "This code is used to create the modifed images with the purple and the yellow squares used to help model 2 to make the difference between commecial and residential buildings. In the \"directories\" folder you have the images labelled for model2, then this code generate the txt files using model1, those are put in a another folder with the images. Then those images are modfied and put in a folder where the json files (the labels for model2) can be found. All actions could be done in the same folder but to avoid any data loss I believe it's better to create separate folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fc69be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_suburbs = YOLO('./runs/detect/train18/weights/best.pt')\n",
    "model1_countryside = YOLO('./rns/detect/train47/weights/best.pt')\n",
    "model1_city = YOLO('./rns/detect/train21/weights/best.pt')\n",
    "models = [model1_countryside, model1_suburbs, model1_city]\n",
    "\n",
    "directories = ['./modele2/images_suburbs_prepro', './modele2/images_city_prepro', './modele2/images_countryside_prepro'] \n",
    "results_directories = ['./modele2/images_suburbs_m2', './modele2/images_city_m2', './modele2/images_countryside_m2']\n",
    "outputs_directories = ['./modele2/data_suburbs_m2', './modele2/data_city_m2', './modele2/data_countryside_m2']\n",
    "\n",
    "### TESTS ###\n",
    "\"\"\"\n",
    "directories = ['./modele2/img_to_process']\n",
    "results_directories = ['./modele2/data_for_processing']\n",
    "outputs_directories = ['./modele2/img_processed']\n",
    "\"\"\"\n",
    "decoding_of_predictions ={0: 'residential', 1: 'commercial'}\n",
    "\n",
    "for k, direct in enumerate(directories) : \n",
    "    generate_txt(direct, results_directories[k], 'png', models, decoding_of_predictions)\n",
    "    copy_images(direct, results_directories[k], 'png')\n",
    "    area_identification_val(results_directories[k], outputs_directories[k], 'png')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c5b9825",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0532915",
   "metadata": {},
   "source": [
    "Now we have images with modifed hue (purple on commercial buldings and yellow on residential buldings). Those images have been labelled beforehand and are now in a folder with their json file. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "660e794c",
   "metadata": {},
   "source": [
    "### Suburbs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "522dcf48",
   "metadata": {},
   "source": [
    "Once again we have to sort the datas in a way yolo will understand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ca8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!labelme2yolo --json_dir ./modele2/data_suburbs_m2\n",
    "\n",
    "# Chemins des répertoires et fichiers\n",
    "source = \"./modele2/data_suburbs_m2/YOLODataset/\"\n",
    "target_dir = \"./modele2/modele2_datasets_suburbs\"\n",
    "\n",
    "labelme_to_yolo(source, target_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6741fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model2_suburbs = YOLO('runs/detect/train34/weights/best.pt')\n",
    "model2_suburbs.train(data='dataset_model2.yaml', epochs= 200, imgsz=512,mixup=0.7, fliplr=0.5, flipud=0.5, copy_paste=0.5, auto_augment='autoaugment', patience=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d75aca04",
   "metadata": {},
   "source": [
    "### City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717cacde",
   "metadata": {},
   "outputs": [],
   "source": [
    "!labelme2yolo --json_dir ./modele2/data_city_m2\n",
    "\n",
    "# Chemins des répertoires et fichiers\n",
    "\n",
    "source = \"./modele2/data_city_m2/YOLODataset/\"\n",
    "target_dir = \"./modele2/modele2_datasets_city\"\n",
    "\n",
    "labelme_to_yolo(source, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065bd6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_city = YOLO('runs/detect/train37/weights/best.pt')\n",
    "model2_city.train(data='dataset_model2.yaml', epochs= 200, imgsz=512,mixup=0.7, fliplr=0.5, flipud=0.5, copy_paste=0.5, auto_augment='autoaugment', patience=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f33ade3b",
   "metadata": {},
   "source": [
    "### Countryside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dd8b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!labelme2yolo --json_dir ./modele2/data_countryside_m2\n",
    "\n",
    "# Chemins des répertoires et fichiers\n",
    "\n",
    "source = \"./modele2/data_countryside_m2/YOLODataset/\"\n",
    "target_dir = \"./modele2/modele2_datasets_countryside\"\n",
    "\n",
    "labelme_to_yolo(source, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2172dfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model2_countryside = YOLO('runs/detect/train27/weights/best.pt')\n",
    "model2_countryside.train(data='dataset_model2.yaml', epochs= 200, imgsz=512,mixup=0.7, fliplr=0.5, flipud=0.5, copy_paste=0.5, auto_augment='autoaugment', patience=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ccd5d91",
   "metadata": {},
   "source": [
    "## Porcédure pour la validation sur les images test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(models1, models2) :\n",
    "    ################ Modèle 1: \n",
    "\n",
    "\n",
    "    directory1 = './VAL/0-IMG_FOR_VAL'\n",
    "    results_directory1 = './VAL/1-RES_M1'\n",
    "    decoding_of_predictions ={0: 'residential', 1: 'commercial'}\n",
    "\n",
    "    generate_txt(directory1, results_directory1, 'png', models, decoding_of_predictions)\n",
    "\n",
    "    ################## Création du dossier à partir duquel on forme les images pour le modèle 2\n",
    "    copy_images('./VAL/0-IMG_FOR_VAL','./VAL/1-RES_M1', 'png')\n",
    "\n",
    "    ################## A partir de ce dossier, fabrication des images à donner au modèle 2\n",
    "    output_dir = './VAL/2-ENTRY_M2'\n",
    "\n",
    "    if os.path.exists(output_dir) : shutil.rmtree(output_dir)\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "    area_identification_val('./VAL/1-RES_M1', output_dir, 'png')\n",
    "    \n",
    "    ###################### Modèle 2\n",
    "\n",
    "\n",
    "    ###################### Prédiction \n",
    "\n",
    "    # Decoding according to the .yaml file class names order\n",
    "    decoding_of_predictions ={0: 'damagedcommercialbuilding', 1: 'undamagedcommercialbuilding', 2: 'undamagedresidentialbuilding', 3: 'damagedresidentialbuilding'}\n",
    "    directory2 = './VAL/2-ENTRY_M2'\n",
    "    # Directory to store outputs\n",
    "    results_directory2 = './VAL/3-FINAL_RES'\n",
    "    generate_txt(directory2, results_directory2, models2, decoding_of_predictions)\n",
    "    \n",
    "    ####################### Mise en ZIP \n",
    "\n",
    "    # Define your source directory and the destination where the zip file will be created\n",
    "    source_dir = './VAL/3-FINAL_RES'\n",
    "    destination_zip = './VAL/submission'\n",
    "\n",
    "    # Create a zip file from the directory\n",
    "    shutil.make_archive(destination_zip, 'zip', source_dir)\n",
    "\n",
    "    print(f\"Directory {source_dir} has been successfully zipped into {destination_zip}.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1c0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_suburbs = YOLO('./runs/detect/train18/weights/best.pt')\n",
    "model1_countryside = YOLO('./rns/detect/train47/weights/best.pt')\n",
    "model1_city = YOLO('./rns/detect/train21/weights/best.pt')\n",
    "models1 = [model1_countryside, model1_suburbs, model1_city]\n",
    "\n",
    "model2_suburbs = YOLO('runs/detect/train34/weights/best.pt')\n",
    "model2_city = YOLO('runs/detect/train37/weights/best.pt')\n",
    "model2_countryside = YOLO('runs/detect/train27/weights/best.pt')\n",
    "models2 = [model2_countryside, model2_suburbs, model2_city]\n",
    "validation(models1, models2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
